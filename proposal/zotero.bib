
@techreport{milani_survey_2022,
	title = {A {Survey} of {Explainable} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2202.08434},
	abstract = {Explainable reinforcement learning (XRL) is an emerging subfield of explainable machine learning that has attracted considerable attention in recent years. The goal of XRL is to elucidate the decision-making process of learning agents in sequential decision-making settings. In this survey, we propose a novel taxonomy for organizing the XRL literature that prioritizes the RL setting. We overview techniques according to this taxonomy. We point out gaps in the literature, which we use to motivate and outline a roadmap for future work.},
	number = {arXiv:2202.08434},
	urldate = {2022-05-19},
	institution = {arXiv},
	author = {Milani, Stephanie and Topin, Nicholay and Veloso, Manuela and Fang, Fei},
	month = feb,
	year = {2022},
	note = {arXiv:2202.08434 [cs]
type: article},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rcleende/Zotero/storage/GGPF45MS/Milani et al. - 2022 - A Survey of Explainable Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/rcleende/Zotero/storage/AMMYUK2V/2202.html:text/html},
}

@techreport{zahavy_graying_2017,
	title = {Graying the black box: {Understanding} {DQNs}},
	shorttitle = {Graying the black box},
	url = {http://arxiv.org/abs/1602.02658},
	abstract = {In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically. The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover, we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.},
	number = {arXiv:1602.02658},
	urldate = {2022-05-19},
	institution = {arXiv},
	author = {Zahavy, Tom and Zrihem, Nir Ben and Mannor, Shie},
	month = apr,
	year = {2017},
	note = {arXiv:1602.02658 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@techreport{weber_knowledge-based_2021,
	title = {Knowledge-based {XAI} through {CBR}: {There} is more to explanations than models can tell},
	shorttitle = {Knowledge-based {XAI} through {CBR}},
	url = {http://arxiv.org/abs/2108.10363},
	abstract = {The underlying hypothesis of knowledge-based explainable artificial intelligence is the data required for data-centric artificial intelligence agents (e.g., neural networks) are less diverse in contents than the data required to explain the decisions of such agents to humans. The idea is that a classifier can attain high accuracy using data that express a phenomenon from one perspective whereas the audience of explanations can entail multiple stakeholders and span diverse perspectives. We hence propose to use domain knowledge to complement the data used by agents. We formulate knowledge-based explainable artificial intelligence as a supervised data classification problem aligned with the CBR methodology. In this formulation, the inputs are case problems composed of both the inputs and outputs of the data-centric agent and case solutions, the outputs, are explanation categories obtained from domain knowledge and subject matter experts. This formulation does not typically lead to an accurate classification, preventing the selection of the correct explanation category. Knowledge-based explainable artificial intelligence extends the data in this formulation by adding features aligned with domain knowledge that can increase accuracy when selecting explanation categories.},
	number = {arXiv:2108.10363},
	urldate = {2022-05-20},
	institution = {arXiv},
	author = {Weber, Rosina and Shrestha, Manil and Johs, Adam J.},
	month = aug,
	year = {2021},
	note = {arXiv:2108.10363 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 12 pages, 8 figures. This paper was accepted at workshop XCBR: Case-Based Reasoning for the Explanation of Intelligent Systems at ICCBR 2021},
	file = {arXiv Fulltext PDF:/Users/rcleende/Zotero/storage/UCB484WQ/Weber et al. - 2021 - Knowledge-based XAI through CBR There is more to .pdf:application/pdf;arXiv.org Snapshot:/Users/rcleende/Zotero/storage/2V54K5B3/2108.html:text/html},
}

@techreport{peng_detecting_2021,
	title = {Detecting and {Adapting} to {Novelty} in {Games}},
	url = {http://arxiv.org/abs/2106.02204},
	abstract = {Open-world novelty occurs when the rules of an environment can change abruptly, such as when a game player encounters "house rules". To address open-world novelty, game playing agents must be able to detect when novelty is injected, and to quickly adapt to the new rules. We propose a model-based reinforcement learning approach where game state and rules are represented as knowledge graphs. The knowledge graph representation of the state and rules allows novelty to be detected as changes in the knowledge graph, assists with the training of deep reinforcement learners, and enables imagination-based re-training where the agent uses the knowledge graph to perform look-ahead.},
	number = {arXiv:2106.02204},
	urldate = {2022-05-20},
	institution = {arXiv},
	author = {Peng, Xiangyu and Balloch, Jonathan C. and Riedl, Mark O.},
	month = jun,
	year = {2021},
	doi = {10.48550/arXiv.2106.02204},
	note = {arXiv:2106.02204 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 10 pages, 5 figures, Accepted to the AAAI21 Workshop on on Reinforcement Learning in Games},
	file = {arXiv Fulltext PDF:/Users/rcleende/Zotero/storage/3PVHTK6W/Peng et al. - 2021 - Detecting and Adapting to Novelty in Games.pdf:application/pdf;arXiv.org Snapshot:/Users/rcleende/Zotero/storage/MKANL2BV/2106.html:text/html},
}

@techreport{ehsan_rationalization_2017,
	title = {Rationalization: {A} {Neural} {Machine} {Translation} {Approach} to {Generating} {Natural} {Language} {Explanations}},
	shorttitle = {Rationalization},
	url = {http://arxiv.org/abs/1702.07826},
	abstract = {We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.},
	number = {arXiv:1702.07826},
	urldate = {2022-05-20},
	institution = {arXiv},
	author = {Ehsan, Upol and Harrison, Brent and Chan, Larry and Riedl, Mark O.},
	month = dec,
	year = {2017},
	doi = {10.48550/arXiv.1702.07826},
	note = {arXiv:1702.07826 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	annote = {Comment: 9 pages, 4 figures; added human evaluation section; added author; changed author order-Upol Ehsan and Brent Harrison both contributed equally to this work},
	file = {arXiv Fulltext PDF:/Users/rcleende/Zotero/storage/EQE2JMUM/Ehsan et al. - 2017 - Rationalization A Neural Machine Translation Appr.pdf:application/pdf;arXiv.org Snapshot:/Users/rcleende/Zotero/storage/WV2BRFHL/1702.html:text/html},
}

@techreport{goel_unsupervised_2018,
	title = {Unsupervised {Video} {Object} {Segmentation} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1805.07780},
	abstract = {We present a new technique for deep reinforcement learning that automatically detects moving objects and uses the relevant information for action selection. The detection of moving objects is done in an unsupervised way by exploiting structure from motion. Instead of directly learning a policy from raw images, the agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the policy of the agent on the moving objects. Over time, the agent identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, which we call Motion-Oriented REinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Furthermore, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. We can gain insight into the policy by inspecting the segmentation and motion of each object detected by the agent. This allows practitioners to confirm whether a policy is making decisions based on sensible information.},
	number = {arXiv:1805.07780},
	urldate = {2022-05-20},
	institution = {arXiv},
	author = {Goel, Vik and Weng, Jameson and Poupart, Pascal},
	month = may,
	year = {2018},
	doi = {10.48550/arXiv.1805.07780},
	note = {arXiv:1805.07780 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rcleende/Zotero/storage/L93WMI27/Goel et al. - 2018 - Unsupervised Video Object Segmentation for Deep Re.pdf:application/pdf;arXiv.org Snapshot:/Users/rcleende/Zotero/storage/7RLWRZFE/1805.html:text/html},
}

@techreport{greydanus_visualizing_2018,
	title = {Visualizing and {Understanding} {Atari} {Agents}},
	url = {http://arxiv.org/abs/1711.00138},
	abstract = {While deep reinforcement learning (deep RL) agents are effective at maximizing rewards, it is often unclear what strategies they use to do so. In this paper, we take a step toward explaining deep RL agents through a case study using Atari 2600 environments. In particular, we focus on using saliency maps to understand how an agent learns and executes a policy. We introduce a method for generating useful saliency maps and use it to show 1) what strong agents attend to, 2) whether agents are making decisions for the right or wrong reasons, and 3) how agents evolve during learning. We also test our method on non-expert human subjects and find that it improves their ability to reason about these agents. Overall, our results show that saliency information can provide significant insight into an RL agent's decisions and learning behavior.},
	number = {arXiv:1711.00138},
	urldate = {2022-05-20},
	institution = {arXiv},
	author = {Greydanus, Sam and Koul, Anurag and Dodge, Jonathan and Fern, Alan},
	month = sep,
	year = {2018},
	doi = {10.48550/arXiv.1711.00138},
	note = {arXiv:1711.00138 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: ICML 2018 conference paper. Code: https://github.com/greydanus/visualize\_atari Blog: https://greydanus.github.io/2017/11/01/visualize-atari/},
	file = {arXiv Fulltext PDF:/Users/rcleende/Zotero/storage/PWYHZXUP/Greydanus et al. - 2018 - Visualizing and Understanding Atari Agents.pdf:application/pdf;arXiv.org Snapshot:/Users/rcleende/Zotero/storage/EEE9VIRD/1711.html:text/html},
}

@article{tiddi_knowledge_2022,
	title = {Knowledge graphs as tools for explainable machine learning: {A} survey},
	volume = {302},
	issn = {0004-3702},
	shorttitle = {Knowledge graphs as tools for explainable machine learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221001788},
	doi = {10.1016/j.artint.2021.103627},
	abstract = {This paper provides an extensive overview of the use of knowledge graphs in the context of Explainable Machine Learning. As of late, explainable AI has become a very active field of research by addressing the limitations of the latest machine learning solutions that often provide highly accurate, but hardly scrutable and interpretable decisions. An increasing interest has also been shown in the integration of Knowledge Representation techniques in Machine Learning applications, mostly motivated by the complementary strengths and weaknesses that could lead to a new generation of hybrid intelligent systems. Following this idea, we hypothesise that knowledge graphs, which naturally provide domain background knowledge in a machine-readable format, could be integrated in Explainable Machine Learning approaches to help them provide more meaningful, insightful and trustworthy explanations. Using a systematic literature review methodology we designed an analytical framework to explore the current landscape of Explainable Machine Learning. We focus particularly on the integration with structured knowledge at large scale, and use our framework to analyse a variety of Machine Learning domains, identifying the main characteristics of such knowledge-based, explainable systems from different perspectives. We then summarise the strengths of such hybrid systems, such as improved understandability, reactivity, and accuracy, as well as their limitations, e.g. in handling noise or extracting knowledge efficiently. We conclude by discussing a list of open challenges left for future research.},
	language = {en},
	urldate = {2022-05-20},
	journal = {Artificial Intelligence},
	author = {Tiddi, Ilaria and Schlobach, Stefan},
	month = jan,
	year = {2022},
	keywords = {Explainable AI, Explainable systems, Explanations, Knowledge graphs, Neuro-symbolic integration, Subsymbolic AI, Symbolic AI},
	pages = {103627},
	file = {ScienceDirect Snapshot:/Users/rcleende/Zotero/storage/QAEC988A/S0004370221001788.html:text/html},
}

@article{Olson_2021,
	doi = {10.1016/j.artint.2021.103455},
  
	url = {https://doi.org/10.1016%2Fj.artint.2021.103455},
  
	year = 2021,
	month = {jun},
  
	publisher = {Elsevier {BV}
},
  
	volume = {295},
  
	pages = {103455},
  
	author = {Matthew L. Olson and Roli Khanna and Lawrence Neal and Fuxin Li and Weng-Keen Wong},
  
	title = {Counterfactual state explanations for reinforcement learning agents via generative deep learning},
  
	journal = {Artificial Intelligence}
}

@misc{Oltramari2020,
  doi = {10.48550/ARXIV.2003.04707},
  
  url = {https://arxiv.org/abs/2003.04707},
  
  author = {Oltramari, Alessandro and Francis, Jonathan and Henson, Cory and Ma, Kaixin and Wickramarachchi, Ruwan},
  
  keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Symbolic Computation (cs.SC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neuro-symbolic Architectures for Context Understanding},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
