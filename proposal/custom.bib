% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@misc{Puiutta2020,
  doi = {10.48550/ARXIV.2005.06247},
  
  url = {https://arxiv.org/abs/2005.06247},
  
  author = {Puiutta, Erika and Veith, Eric MSP},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, A.1},
  
  title = {Explainable Reinforcement Learning: A Survey},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Hoffman2018,
  doi = {10.48550/ARXIV.1812.04608},
  
  url = {https://arxiv.org/abs/1812.04608},
  
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Metrics for Explainable AI: Challenges and Prospects},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Araujo2020,
author = {Araujo, Theo and Helberger, Natali and Kruikemeier, Sanne and de Vreese, Claes H.},
title = {In AI We Trust? Perceptions about Automated Decision-Making by Artificial Intelligence},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {3},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-019-00931-w},
doi = {10.1007/s00146-019-00931-w},
abstract = {Fueled by ever-growing amounts of (digital) data and advances in artificial intelligence, decision-making in contemporary societies is increasingly delegated to automated processes. Drawing from social science theories and from the emerging body of research about algorithmic appreciation and algorithmic perceptions, the current study explores the extent to which personal characteristics can be linked to perceptions of automated decision-making by AI, and the boundary conditions of these perceptions, namely the extent to which such perceptions differ across media, (public) health, and judicial contexts. Data from a scenario-based survey experiment with a national sample (N = 958) show that people are by and large concerned about risks and have mixed opinions about fairness and usefulness of automated decision-making at a societal level, with general attitudes influenced by individual characteristics. Interestingly, decisions taken automatically by AI were often evaluated on par or even better than human experts for specific decisions. Theoretical and societal implications about these findings are discussed.},
journal = {AI Soc.},
month = {sep},
pages = {611–623},
numpages = {13},
keywords = {Algorithmic appreciation, Algorithmic fairness, Artificial intelligence, User perceptions, Automated decision-making}
}

@article{TIDDI2022103627,
title = {Knowledge graphs as tools for explainable machine learning: A survey},
journal = {Artificial Intelligence},
volume = {302},
pages = {103627},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103627},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221001788},
author = {Ilaria Tiddi and Stefan Schlobach},
keywords = {Explainable systems, Knowledge graphs, Explanations, Symbolic AI, Subsymbolic AI, Neuro-symbolic integration, Explainable AI},
abstract = {This paper provides an extensive overview of the use of knowledge graphs in the context of Explainable Machine Learning. As of late, explainable AI has become a very active field of research by addressing the limitations of the latest machine learning solutions that often provide highly accurate, but hardly scrutable and interpretable decisions. An increasing interest has also been shown in the integration of Knowledge Representation techniques in Machine Learning applications, mostly motivated by the complementary strengths and weaknesses that could lead to a new generation of hybrid intelligent systems. Following this idea, we hypothesise that knowledge graphs, which naturally provide domain background knowledge in a machine-readable format, could be integrated in Explainable Machine Learning approaches to help them provide more meaningful, insightful and trustworthy explanations. Using a systematic literature review methodology we designed an analytical framework to explore the current landscape of Explainable Machine Learning. We focus particularly on the integration with structured knowledge at large scale, and use our framework to analyse a variety of Machine Learning domains, identifying the main characteristics of such knowledge-based, explainable systems from different perspectives. We then summarise the strengths of such hybrid systems, such as improved understandability, reactivity, and accuracy, as well as their limitations, e.g. in handling noise or extracting knowledge efficiently. We conclude by discussing a list of open challenges left for future research.}
}

@misc{Xu2015,
  doi = {10.48550/ARXIV.1502.03044},
  url = {https://arxiv.org/abs/1502.03044},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Ribeiro_Singh_Guestrin_2018, 
  title={Anchors: High-Precision Model-Agnostic Explanations}, 
  volume={32}, 
  url={https://ojs.aaai.org/index.php/AAAI/article/view/11491}, 
  abstractNote={ &lt;p&gt; We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, &quot;sufficient&quot; conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations. &lt;/p&gt; }, 
  number={1}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos}, 
  year={2018}, 
  month={Apr.} 
}

@inproceedings{ijcai2019-876,
  title     = {Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning},
  author    = {Byrne, Ruth M. J.},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {6276--6282},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/876},
  url       = {https://doi.org/10.24963/ijcai.2019/876},
}

@ARTICLE{Wells2021,
  AUTHOR={Wells, Lindsay and Bednarz, Tomasz},   
  TITLE={Explainable AI and Reinforcement Learning—A Systematic Review of Current Approaches and Trends},      
  JOURNAL={Frontiers in Artificial Intelligence},      
  VOLUME={4},      
  YEAR={2021},      
  URL={https://www.frontiersin.org/article/10.3389/frai.2021.550030},       
  DOI={10.3389/frai.2021.550030},      
  ISSN={2624-8212},   
  ABSTRACT={Research into Explainable Artificial Intelligence (XAI) has been increasing in recent years as a response to the need for increased transparency and trust in AI. This is particularly important as AI is used in sensitive domains with societal, ethical, and safety implications. Work in XAI has primarily focused on Machine Learning (ML) for classification, decision, or action, with detailed systematic reviews already undertaken. This review looks to explore current approaches and limitations for XAI in the area of Reinforcement Learning (RL). From 520 search results, 25 studies (including 5 snowball sampled) are reviewed, highlighting visualization, query-based explanations, policy summarization, human-in-the-loop collaboration, and verification as trends in this area. Limitations in the studies are presented, particularly a lack of user studies, and the prevalence of toy-examples and difficulties providing understandable explanations. Areas for future study are identified, including immersive visualization, and symbolic representation.}
}

@misc{Milani2022,
  doi = {10.48550/ARXIV.2202.08434},
  url = {https://arxiv.org/abs/2202.08434},
  author = {Milani, Stephanie and Topin, Nicholay and Veloso, Manuela and Fang, Fei},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Survey of Explainable Reinforcement Learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}